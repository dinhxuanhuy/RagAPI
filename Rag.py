import os
import time
from dotenv import load_dotenv
from langchain_qdrant import QdrantVectorStore
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from langchain_classic.chains import RetrievalQAWithSourcesChain
from langchain_classic.prompts import PromptTemplate

# Load environment variables
load_dotenv()

class RagPipeLine():
    def __init__(self):
        self.model_name = os.getenv("model_name", "BAAI/bge-m3")
        self.qdrant_url = os.getenv("QDRANT_URL")
        self.qdrant_api_key = os.getenv("QDRANT_API_KEY")
        self.collection_name = os.getenv("COLLECTION_NAME", "Phone_store")
        self.google_api_key = os.getenv("GOOGLE_API_KEY")

        if not all([self.qdrant_url, self.qdrant_api_key, self.google_api_key]):
            raise ValueError("Missing required environment variables: QDRANT_URL, QDRANT_API_KEY, GOOGLE_API_KEY")

        self.embeddings = self.load_embeddings()
        self.retriever = self.load_retriever(embeddings=self.embeddings)
        self.pipe = self.load_model_pipeline(max_new_tokens=300)
        self.prompt = self.load_prompt_template()
        self.rag_pipeline = self.load_rag_pipeline(llm=self.pipe,
                                            retriever=self.retriever,
                                            prompt=self.prompt)
        print("RAG Pipeline initialized successfully.")

    def load_embeddings(self):
        embeddings = HuggingFaceEmbeddings(model_name=self.model_name)
        return embeddings

    def load_retriever(self, embeddings):
        client = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_api_key, prefer_grpc=False, check_compatibility=False)
        db = QdrantVectorStore(
            client=client,
            collection_name=self.collection_name,
            embedding=embeddings,
        )
        return db.as_retriever(search_kwargs={"k": 15})

    def load_model_pipeline(self, max_new_tokens=300):
        pipe = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash-lite", 
            temperature=0.1,
            top_p=0.8,
            max_output_tokens=max_new_tokens,
            google_api_key=self.google_api_key
        )
        return pipe

    def get__LLM__answer(self, question):
        response = self.pipe.invoke(question)
        return response

    def load_prompt_template(self):
        PROMPT_TEMPLATE = """
        B·∫°n l√† chuy√™n gia t∆∞ v·∫•n ƒëi·ªán tho·∫°i chuy√™n nghi·ªáp v·ªõi kinh nghi·ªám 10 nƒÉm t·∫°i Thegioididong/Cellphones.

        D·ª±a v√†o **TH√îNG S·ªê K·ª∏ THU·∫¨T C·ª§ TH·ªÇ** c·ªßa t·ª´ng m·∫´u ƒëi·ªán tho·∫°i d∆∞·ªõi ƒë√¢y v√† ki·∫øn th·ª©c chuy√™n m√¥n v·ªÅ th·ªã tr∆∞·ªùng Vi·ªát Nam, h√£y t∆∞ v·∫•n ch√¢n th·ª±c, kh√°ch quan.

        {context}

        ---

        **KH√ÅCH H√ÄNG H·ªéI**: {question}

        **HU·ªöNG D·∫™N T∆Ø V·∫§N**:
        1. **PH√ÇN T√çCH ∆ØU/ NH∆Ø·ª¢C** t·ª´ specs (pin th·ª±c t·∫ø, hi·ªáu nƒÉng gaming, camera th·ª±c chi·∫øn)
        2. **SO S√ÅNH** v·ªõi ƒë·ªëi th·ªß c√πng t·∫ßm gi√° (iPhone vs Samsung vs Xiaomi)
        3. **PH√ô H·ª¢P NHU C·∫¶U**: gaming/streaming/ch·ª•p ·∫£nh/pin tr√¢u/d∆∞·ªõi Xtr
        4. **GI√Å TR·ªä TI·ªÄN B·∫†C**: ƒê√°ng mua hay ch·ªù sale?
        5. **KI·∫æN TH·ª®C B·ªî SUNG**: Benchmark th·ª±c t·∫ø, ƒë·ªô b·ªÅn VN, ch√≠nh s√°ch b·∫£o h√†nh

        **TR·∫¢ L·ªúI**:
        ‚úÖ Ng·∫Øn g·ªçn, thuy·∫øt ph·ª•c nh∆∞ sales pro
        ‚úÖ Bullet points r√µ r√†ng  
        ‚úÖ K·∫øt lu·∫≠n "N√™n mua/Kh√¥ng n√™n/ƒê·ª£i model m·ªõi"
        ‚úÖ Gi√° VND, so s√°nh local market

        B·∫Øt ƒë·∫ßu t∆∞ v·∫•n üëá
        """
        input_variables = ["context", "question"]
        prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=input_variables)
        return prompt

    def load_rag_pipeline(self, llm, retriever, prompt):
        doc_prompt = PromptTemplate(
            input_variables=["page_content"],
            template="{page_content}",
        )
        rag_pipeline = RetrievalQAWithSourcesChain.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=False,
            chain_type_kwargs={
                "prompt": prompt,
                "document_variable_name": "context",
                "document_prompt": doc_prompt,
            },
        )
        return rag_pipeline


    def rag_ask(self, question):
        max_retries = 3
        retry_delay = 30  # seconds
        
        for attempt in range(max_retries):
            try:
                # Use invoke instead of __call__ to avoid deprecation warning
                return self.rag_pipeline.invoke({"question": question})
            except Exception as e:
                error_str = str(e)
                if "429" in error_str and "RESOURCE_EXHAUSTED" in error_str:
                    if attempt < max_retries - 1:
                        print(f"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})")
                        time.sleep(retry_delay)
                        continue
                
                print(f"Error in rag_ask: {e}")
                raise e
